{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning with Optuna\n","Optimize XGBoost hyperparameters using Bayesian optimization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import optuna\nfrom optuna.pruners import MedianPruner\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n    }\n    \n    model = xgb.XGBRegressor(**params, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n    return scores.mean()\n\nstudy = optuna.create_study(direction='maximize', pruner=MedianPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(f'Best R² Score: {study.best_value:.4f}')\nprint(f'Best Parameters: {study.best_params}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize optimization history\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\ntrial_values = [trial.value for trial in study.trials if trial.value is not None]\nplt.plot(trial_values, marker='o', alpha=0.6)\nplt.xlabel('Trial')\nplt.ylabel('R² Score')\nplt.title('Optuna Optimization History')\nplt.grid(True)\nplt.show()\n\n# Parameter importance\nfig = optuna.visualization.plot_param_importances(study).to_dict()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}
